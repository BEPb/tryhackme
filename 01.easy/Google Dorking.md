[>> вернуться на главную страницу](https://github.com/BEPb/tryhackme/blob/master/README.md)

# Комната [Google Dorking](https://tryhackme.com/r/room/googledorking) 

Всего 6 заданий:
## Задание 1
Google, пожалуй, самый известный пример «поисковых систем». Кто помнит Ask Jeeves? Вздрагивает.

Сейчас это может показаться довольно покровительственным объяснением того, как работают эти «поисковые системы», но 
за кулисами происходит гораздо больше, чем то, что мы видим. Что еще важнее, мы можем использовать это в своих 
интересах, чтобы находить всевозможные вещи, которые не смог бы найти список слов.  Исследование в целом — особенно 
в контексте кибербезопасности — охватывает почти все, что вы делаете как пентестер. MuirlandOracle  создала 
фантастическую комнату для изучения отношения к тому, как проводить исследования, и какую именно информацию вы 
можете из них получить.

«Поисковые системы», такие как Google, являются крупными индексаторами, в частности, индексаторами контента, 
распространенного по всей Всемирной паутине.

Эти основные элементы интернет-серфинга используют «ползунков» или «пауков» для поиска контента во Всемирной паутине,
о чем я расскажу в следующем задании.

### Ответить на вопросы ниже
Роджер Доджер!
```commandline
Ответ не нужен
```

## Задание 2
### Что такое краулеры и как они работают?
Эти сканеры обнаруживают контент различными способами. Один из них — чистое обнаружение, когда сканер посещает 
URL-адрес, и информация о типе контента веб-сайта возвращается поисковой системе. На самом деле, современные сканеры 
собирают много информации, но мы обсудим, как это используется, позже. Другой метод, который сканеры используют для 
обнаружения контента, — это отслеживание любых и всех URL-адресов, найденных на ранее просканированных веб-сайтах. 
Очень похоже на вирус в том смысле, что он захочет пройти/распространиться на все, что сможет.


### Давайте представим некоторые вещи...
Диаграмма ниже представляет собой высокоуровневую абстракцию того, как работают эти веб-краулеры. Как только 
веб-краулер обнаруживает домен, такой как mywebsite.com, он индексирует все содержимое домена, ища ключевые слова и 
другую разнообразную информацию, но я обсужу эту разнообразную информацию позже.

На диаграмме выше « mywebsite.com » был просканирован как имеющий ключевые слова «Apple», «Banana» и «Pear». Эти 
ключевые слова сохраняются в словаре сканером, который затем возвращает их поисковой системе, т. е. Google. 
Благодаря этой настойчивости Google теперь знает, что домен « mywebsite.com » имеет ключевые слова «Apple», «Banana» 
и «Pear». Поскольку был просканирован только один веб-сайт, если пользователь будет искать «Apple»... появится  « 
mywebsite.com ». Это приведет к такому же поведению, если пользователь будет искать «Banana». Поскольку 
проиндексированное содержимое сканера сообщает, что домен имеет «Banana», он будет отображаться для пользователя.


Как показано ниже, пользователь отправляет запрос в поисковую систему по запросу «Pears». Поскольку поисковая 
система имеет содержимое только одного веб-сайта, который был просканирован по ключевому слову «Pears», это будет 
единственный домен, представленный пользователю.   



Однако, как мы уже упоминали, сканеры пытаются обойти, что называется сканированием, каждый URL и файл, которые они 
могут найти! Скажем, если у « mywebsite.com » были те же ключевые слова, что и раньше («Apple», «Banana» и «Pear»), 
но также был URL на другой веб-сайт « otherwebsite.com », сканер попытается обойти все на этом URL ( anotherwebsite.
com ) и извлечь содержимое всего в этом домене соответственно.   


Это показано на схеме ниже. Сначала сканер находит « mywebsite.com », где он сканирует содержимое веб-сайта, находя 
те же ключевые слова («Apple», «Banana» и «Pear»), что и раньше, но он дополнительно нашел внешний URL. После того, 
как сканер завершит работу с « mywebsite.com », он продолжит сканировать содержимое веб-сайта « otherwebsite.com », 
где на нем найдены ключевые слова («Tomatoes», «Strawberries» и «Pineapples»). Словарь сканера теперь содержит 
содержимое как « mywebsite.com », так и « otherwebsite.com », которое затем сохраняется и сохраняется в поисковой 
системе.     

### Подведение итогов

Итак, подведем итог: поисковая система теперь знает о двух просканированных доменах:
1. mywebsite.com
2. anotherwebsite.com

Хотя следует отметить, что « anotherwebsite.com » был просканирован только потому, что на него ссылался первый домен 
« mywebsite.com ». Благодаря этой ссылке поисковая система знает следующее о двух доменах: 

>Доменное имя	Ключевое слово
мойсайт.com	Яблоки
мойсайт.com
Бананы
мойсайт.com
Груши
anotherwebsite.com	Помидоры
anotherwebsite.com	Клубника
anotherwebsite.com	Ананасы
Или как показано ниже:


Теперь, когда поисковая система имеет некоторые знания о ключевых словах, скажем, если пользователь ищет «Pears», 
будет отображен домен « mywebsite.com », поскольку это единственный просканированный домен, содержащий «Pears»: 



Аналогично, скажем, в этом случае пользователь сейчас ищет "Strawberries". Домен " anotherwebsite.com " будет 
отображен, так как это единственный домен, который был просканирован поисковой системой и содержит ключевое слово 
"Strawberries":  


Это здорово... Но представьте, если бы у веб-сайта было несколько внешних URL-адресов (как это часто бывает!). Это 
потребовало бы большого сканирования. Всегда есть вероятность, что другой веб-сайт может иметь похожую информацию, 
как и тот, который был просканирован другим веб-сайтом - верно? Так как же "поисковая система" определяет иерархию 
доменов, которые отображаются для пользователя?   

В приведенной ниже диаграмме в данном случае, если пользователь ищет ключевое слово, например, «Помидоры» (которое 
содержат веб-сайты 1–3), кто решает, какой веб-сайт и в каком порядке отображать? 


Логичным предположением было бы, что будет отображаться веб-сайт 1 -> 3... Но настоящие домены работают и/или 
именуются не так. 

Итак, кто (или что) определяет иерархию? Ну...

### Ответить на вопросы ниже
Назовите ключевой термин, для чего используется «Crawler»
```commandline
Index
```
Как называется метод, который используют «поисковые системы» для получения информации о веб-сайтах?
```commandline
Crawling
```
Какой пример контента можно получить с веб-сайта?
```commandline
Keywords
```

## Задание 3
### Поисковая оптимизация
Поисковая оптимизация или SEO — это распространенная и прибыльная тема в современных поисковых системах. Фактически, 
настолько распространенная, что целые компании извлекают выгоду из улучшения SEO-рейтинга доменов. С абстрактной 
точки зрения поисковые системы будут «отдавать приоритет» тем доменам, которые легче индексировать. Существует много 
факторов, определяющих «оптимальность» домена, что приводит к чему-то похожему на систему баллов.


Чтобы выделить несколько факторов, влияющих на то, как начисляются эти баллы, можно выделить следующие:
• Насколько ваш сайт адаптируется к различным типам браузеров, например, Google Chrome, Firefox и Internet Explorer, включая мобильные телефоны!
• Насколько легко сканировать ваш веб-сайт (или разрешено ли вообще сканирование... но мы вернемся к этому позже) с помощью «Файлов Sitemap»
• Какие ключевые слова есть на вашем сайте (например, в наших примерах, если пользователь ищет запрос типа «Цвета», 
домен не будет возвращен, так как поисковая система (еще) не просканировала домен, в котором есть какие-либо 
ключевые слова, связанные с «Цветами»).  



Существует много сложностей в том, как различные поисковые системы индивидуально "оценивают" или ранжируют эти 
домены - включая обширные алгоритмы. Естественно, компании, управляющие этими поисковыми системами, такие как Google,
не делятся тем, как именно в конечном итоге выглядит иерархическое представление доменов. Хотя, поскольку это бизнес 
в конце дня, вы можете заплатить за рекламу/повышение порядка, в котором отображается ваш домен.   



Существуют различные онлайн-инструменты, иногда предоставляемые самими поставщиками поисковых систем, которые 
покажут вам, насколько оптимизирован ваш домен. Например, давайте используем Site Analyzer от Google, чтобы 
проверить рейтинг TryHackMe :  


Согласно этому инструменту, TryHackMe имеет рейтинг SEO 85/100 (по состоянию на 14/11/2020). Это не так уж плохо, и 
ниже на странице будут показаны обоснования того, как была рассчитана эта оценка. 

Но...Кто или что регулирует этих «ползунков»?

Помимо поисковых систем, которые предоставляют эти "Crawlers", владельцы веб-сайтов/веб-серверов в конечном итоге 
сами определяют, какой контент "Crawlers" могут скрейпить. Поисковые системы захотят получить все с веб-сайта, но 
есть несколько случаев, когда мы не хотим, чтобы все содержимое нашего веб-сайта индексировалось! Можете ли вы 
придумать что-нибудь...? Как насчет секретной страницы входа администратора? Мы не хотим, чтобы каждый мог найти 
этот каталог, особенно через поиск Google.    

Представляем Robots.txt... 

### Ответить на вопросы ниже
Используйте тот же инструмент проверки SEO и другие онлайн-альтернативы, чтобы сравнить их результаты для 
https://tryhackme.com и http://googledorking.cmnatic.co.uk 
```commandline
Ответ не нужен
```

## Задание 4
Роботы.txt

Подобно «Sitemaps», которые мы обсудим позже, этот файл первым индексируется «сканерами» при посещении веб-сайта.



Но что это?

Этот файл должен обслуживаться в корневом каталоге, указанном самим веб-сервером. Глядя на расширение этого файла .txt , можно с уверенностью предположить, что это текстовый файл.

Текстовый файл определяет разрешения, которые "Crawler" имеет на веб-сайте. Например, какой тип "Crawler" разрешен (т. е. вы хотите, чтобы ваш сайт индексировал только "Crawler" Google, а не MSN). Более того, Robots.txt может указывать, какие файлы и каталоги мы хотим или не хотим индексировать "Crawler".

Самая простая разметка Robots.txt выглядит следующим образом:





Вот несколько ключевых слов...

Ключевое слово	Функция
Пользователь-агент	Укажите тип «Паука», который может индексировать ваш сайт (звездочка является подстановочным знаком, разрешающим все «User-agents»).
Позволять	Укажите каталоги или файлы, которые «Паук»  может индексировать.
Запретить	Укажите каталоги или файлы, которые «Паук»  не может индексировать
Карта сайта	Укажите, где находится карта сайта (улучшает SEO, как обсуждалось ранее, мы рассмотрим карты сайта в следующей задаче)


В этом случае:

1. Любой «краулер» может индексировать сайт

2. «Пауку» разрешено индексировать все содержимое сайта.

3. «Карта сайта» находится по адресу http://mywebsite.com/sitemap.xml



Допустим, мы хотим скрыть каталоги или файлы от "Crawler"? Robots.txt работает по принципу "черного списка". По сути, если не указано иное , Crawler индексирует все, что может найти.



В этом случае:

1. Любой «краулер» может индексировать сайт

2. «Паук» может индексировать любой другой контент, который не содержится в «/super-secret-directory/».

Краулеры также знают различия между подкаталогами, каталогами и файлами. Например, в случае второго "Disallow:" ( "/not-a-secret/but-this-is/")

«Паук» проиндексирует все содержимое в « /not-a-secret/ », но не будет индексировать ничего, что содержится в подкаталоге «/but-this-is/» .

3. «Карта сайта» находится по адресу  http://mywebsite.com/sitemap.xml



Что, если бы мы хотели, чтобы наш сайт индексировали только определенные «пауки»?

Мы можем это оговорить, как на рисунке ниже:



В этом случае:

1. «Пауку» «Googlebot» разрешено индексировать весь сайт («Разрешить: /»).

2. "Пауку" "msnbot" не разрешено индексировать сайт (Disallow: /")



Как насчет предотвращения индексации файлов? 

Хотя вы можете вручную вводить данные для каждого расширения файла, которое вы не хотите индексировать, вам придется указать каталог, в котором он находится, а также полное имя файла. Представьте, что у вас огромный сайт! Какая боль...Вот где мы можем использовать немного регулярных выражений .



В этом случае:

1. Любой «краулер» может индексировать сайт

2. Однако «Паук» не может индексировать файлы с расширением .ini в любом каталоге/подкаталоге, использующем («$») сайта.

3.  «Карта сайта» находится по адресу  http://mywebsite.com/sitemap.xml

Зачем вам , например, скрывать файл .ini ? Ну, такие файлы содержат конфиденциальные данные конфигурации. Можете ли вы вспомнить какие-либо другие форматы файлов, которые могут содержать конфиденциальную информацию?



### Ответить на вопросы ниже
Где будет располагаться «robots.txt» на домене « ablog.com »
```commandline
ablog.com/robots.txt
```
Если бы у веб-сайта была карта сайта, где бы она располагалась?
```commandline
/sitemap.xml
```
Как разрешить индексировать сайт только «Bingbot»?
```commandline
User-agent: Bingbot
```
Как нам запретить «Пауку» индексировать каталог «/dont-index-me/»?
```commandline
Disallow: /dont-index-me/
```
Какое расширение файла конфигурации системы Unix/Linux мы можем захотеть скрыть от «сканеров»?
```commandline
.conf
```

## Задание 5
### Карты сайта
«Sitemaps» можно сравнить с географическими картами в реальной жизни, но они предназначены для веб-сайтов!

«Sitemaps» — это индикативные ресурсы, которые полезны для сканеров, поскольку они указывают необходимые маршруты 
для поиска контента на домене. Иллюстрация ниже — хороший пример структуры веб-сайта и того, как он может выглядеть 
на «Sitemap»:  


Синие прямоугольники представляют собой путь к вложенному контенту, похожему на каталог, например «Продукты» для 
магазина. В то время как зеленые скругленные прямоугольники представляют собой реальную страницу. Однако это только 
для иллюстрации — «Карты сайта» не выглядят так в реальном мире. Они выглядят примерно так:


«Sitemaps» отформатированы в XML. Я не буду объяснять структуру этого форматирования файла, так как комната XXE,  
созданная falconfeast,  прекрасно справляется с этой задачей. 

Наличие "Sitemaps" имеет достаточно большой вес в плане влияния на "оптимизацию" и благоприятность веб-сайта. Как мы 
обсуждали в задаче "Search Engine Optimisation", эти карты значительно облегчают обход контента для краулера! 



Почему файлы Sitemap так популярны у поисковых систем?

Поисковые системы ленивы! Ну, еще лучше — поисковым системам нужно обработать много данных. Эффективность сбора этих 
данных имеет первостепенное значение. Такие ресурсы, как «Sitemaps», чрезвычайно полезны для «Crawlers», поскольку 
необходимые маршруты к контенту уже предоставлены! Все, что нужно сделать краулеру, — это соскрести этот контент, а 
не выполнять процесс ручного поиска и соскрести. Подумайте об этом как об использовании списка слов для поиска 
файлов вместо случайного угадывания их имен!



Чем проще веб-сайт для «сканирования», тем он более оптимизирован для «поисковой системы».

### Ответить на вопросы ниже
Какова типичная структура файла «Sitemap»?
```commandline
XML
```
С каким примером из реальной жизни можно сравнить «Sitemaps»?
```commandline
Map
```
Назовите ключевое слово для пути к контенту на веб-сайте.
```commandline
Route
```

## Задание 6
#### Использование Google для расширенного поиска

Как мы уже обсуждали ранее, Google просканировал и проиндексировал множество веб-сайтов. Среднестатистический Джо 
использует Google для поиска фотографий кошек (я сам больше люблю собак...). Хотя Google проиндексирует множество 
фотографий кошек, готовых к показу Джо, это довольно тривиальное использование поисковой системы по сравнению с тем, 
для чего ее можно использовать.   
Например, мы можем добавлять операторы, такие как этот, из языков программирования, чтобы увеличивать или уменьшать 
результаты поиска — или выполнять такие действия, как арифметические!  


Скажем, если мы хотим сузить наш поисковый запрос, мы можем использовать кавычки. Google будет интерпретировать все, 
что находится между этими кавычками, как точное и вернет только результаты точной предоставленной фразы... Довольно 
полезно для фильтрации мусора, который нам не нужен, как мы сделали ниже:  



#### Уточнение наших запросов

Мы можем использовать такие термины, как « site » (например, bbc.co.uk) и запрос (например, «gchq news») для поиска 
указанного сайта по ключевому слову, которое мы предоставили, чтобы отфильтровать контент, который в противном 
случае было бы сложнее найти. Например, используя «site» и «query» «bbc» и «gchq», мы изменили порядок, в котором 
Google возвращает результаты.   

На снимке экрана ниже поиск «gchq news» возвращает около 1 060 000 результатов от Google. Веб-сайт, который мы хотим,
ранжируется ниже фактического веб-сайта GCHQ: 


Но мы этого не хотим... Сначала мы хотели « bbc .co. uk », поэтому давайте уточним наш поиск, используя термин « 
site ». Обратите внимание, как на скриншоте ниже Google возвращает гораздо меньше результатов? Кроме того, страница, 
которая нам не нужна, исчезла, оставив сайт, который нам действительно нужен!   



Конечно, в данном случае GCHQ — это тема для обсуждения, так что в любом случае будет масса результатов.

Так что же делает «гуглодоркинг» таким привлекательным?

Прежде всего — и это самое важное — это законно! Это все индексированная, общедоступная информация. Однако то, что 
вы с этим делаете, — вот где вступает в игру вопрос законности... 

Вот несколько распространенных терминов, которые мы можем искать и комбинировать:
>Срок	Действие
тип файла:
Поиск файла по расширению (например, PDF)
кэш:	Просмотр кэшированной версии Google указанного URL-адреса
название:	Указанная фраза ДОЛЖНА присутствовать в заголовке страницы.
Например, предположим, что мы хотим использовать Google для поиска всех PDF-файлов на bbc.co.uk:

site:bbc.co.uk filetype:pdf


Отлично, теперь мы улучшили наш поиск в Google, чтобы он запрашивал все общедоступные PDF-файлы на « bbc.co.uk ». Вы 
бы не нашли такие файлы, как «Закон о свободе запроса информации», в списке слов! 

Здесь мы использовали расширение PDF, но можете ли вы вспомнить какие-либо другие форматы файлов конфиденциального 
характера, которые могут быть общедоступными? (Часто непреднамеренно!!) Опять же, то, что вы делаете с любыми 
найденными результатами, - это тот случай, когда в игру вступает законность - вот почему "Google Dorking" так 
хорош/опасен.   

### Вот простой обход каталогов.

Я вычеркнул большую часть текста ниже, чтобы охватить вас, себя, THM и владельцев доменов:

### Ответить на вопросы ниже
Какой формат будет использоваться для запроса на сайте bbc.co.uk о защите от наводнений?
```commandline
site: bbc.co.uk flood defences
```
Какой термин вы бы использовали для поиска по типу файла?
```commandline
filetype:
```
Какой термин можно использовать для поиска страниц входа?
```commandline
intitle: login
```

[>> вернуться на главную страницу](https://github.com/BEPb/tryhackme/blob/master/README.md)